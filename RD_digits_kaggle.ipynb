{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RD_digits_kaggle.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7jLtX2giSyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "6dda4720-8d34-4bad-8701-7fc6af60be97"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch import nn\n",
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dt_train_path = \"drive/MyDrive/Kaggle_digits/train.csv\"\n",
        "dt_test_path = \"drive/MyDrive/Kaggle_digits/test.csv\"\n",
        "\n",
        "# open CSV with pandas, then convert to numpy and splice into data and labels\n",
        "dt_train_1d = pd.read_csv(dt_train_path).values\n",
        "dt_train = dt_train_1d[:, 1:]\n",
        "dt_train_labels = dt_train_1d[:, 0]\n",
        "\n",
        "# no labels in test data.\n",
        "dt_test = pd.read_csv(dt_test_path).values\n",
        "\n",
        "print(dt_train.shape)\n",
        "print(dt_test.shape)\n",
        "\n",
        "# convert 784 into 28x28\n",
        "dt_train = np.reshape(dt_train, (dt_train.shape[0], 28, 28))\n",
        "dt_test = np.reshape(dt_test, (dt_test.shape[0], 28, 28))\n",
        "\n",
        "print(dt_train_labels[10])\n",
        "plt.imshow(dt_train[10])\n",
        "plt.show()\n",
        "plt.imshow(dt_test[12])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(42000, 784)\n",
            "(28000, 784)\n",
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "aEUyNhTNAPcQ",
        "outputId": "ccbeb750-e605-46e7-ecbf-9587e3142781"
      },
      "source": [
        "# out = ((n + 2p - k)/s) + 1\n",
        "# \n",
        "class MyModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(1, 8, 3, 1, 1), #28->28\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2, 2)\n",
        "      # nn.BatchNorm2d(8)) #28->14\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(8, 16, 3, 1, 1),           #14->14\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2, 2)\n",
        "        # nn.BatchNorm2d(16)) #14->7\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(16, 32, 3, 1, 1), #7->7\n",
        "        nn.ReLU()\n",
        "        # nn.BatchNorm2d(32)\n",
        "    )\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Linear(7*7*32, 728),\n",
        "        nn.Sigmoid())\n",
        "    self.fc2 = nn.Sequential(\n",
        "        nn.Linear(728, 10),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-100-e8a37c9dbd74>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    self.conv2 = nn.Sequential(\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emZ_JdNVznxv"
      },
      "source": [
        "tensor_train = torch.Tensor(dt_train)\n",
        "tensor_train = tensor_train.unsqueeze(1)\n",
        "tensor_test = torch.Tensor(dt_test)\n",
        "tensor_test = tensor_test.unsqueeze(1)\n",
        "tensor_train_labels = torch.Tensor(dt_train_labels)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(tensor_train, tensor_train_labels)\n",
        "train_dataset, eval_dataset = torch.utils.data.random_split(train_dataset, [int(0.8*len(train_dataset)), int(0.2*len(train_dataset))])\n",
        "data_loader_train = DataLoader(train_dataset, batch_size=1024)\n",
        "\n",
        "# one, two = eval_dataset[:]\n",
        "# print(one)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yLUyn7gXleD",
        "outputId": "52873e0a-ab3f-4e56-cf8c-207a7d4bf67e"
      },
      "source": [
        "model = MyModel()\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "print(model)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyModel(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Sequential(\n",
            "    (0): Linear(in_features=1568, out_features=728, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (fc2): Sequential(\n",
            "    (0): Linear(in_features=728, out_features=10, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n",
            "1155522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i8Q4UDOT-k_r",
        "outputId": "c749e026-d0e8-4b6d-cfaf-62a18e7ae145"
      },
      "source": [
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "  epoch_loss = 0\n",
        "  epoch_accuracy_train = 0\n",
        "  model.train()\n",
        "  for i, (images, labels) in enumerate(data_loader_train):\n",
        "    out = model(images)\n",
        "    loss = loss_function(out, labels.long())\n",
        "    epoch_loss += loss\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total = labels.size(0)\n",
        "    _, pred = torch.max(out.data, 1)\n",
        "    correct = (pred == labels).sum().item()\n",
        "    epoch_accuracy_train += correct/total\n",
        "\n",
        "  model.eval()\n",
        "  x_eval, y_eval = eval_dataset[:]\n",
        "  out = model(x_eval)\n",
        "  _, pred = torch.max(out.data, 1)\n",
        "  correct = (pred == y_eval).sum().item()\n",
        "\n",
        "  print(\"loss: {}\".format(epoch_loss/len(data_loader_train)))\n",
        "  print(\"train acc: {}\".format(epoch_accuracy_train/len(data_loader_train)))\n",
        "  print(\"eval acc: {} \\n\".format(correct/len(y_eval)))\n",
        "  \n",
        "  if (correct/len(y_eval) >= 0.99):\n",
        "    break"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.7256783246994019\n",
            "train acc: 0.8579158471736597\n",
            "eval acc: 0.9536904761904762 \n",
            "\n",
            "loss: 1.5169925689697266\n",
            "train acc: 0.9644863599941725\n",
            "eval acc: 0.9727380952380953 \n",
            "\n",
            "loss: 1.4927674531936646\n",
            "train acc: 0.9779852309149184\n",
            "eval acc: 0.9797619047619047 \n",
            "\n",
            "loss: 1.483755111694336\n",
            "train acc: 0.9842111013986015\n",
            "eval acc: 0.9821428571428571 \n",
            "\n",
            "loss: 1.4785819053649902\n",
            "train acc: 0.9878487398018648\n",
            "eval acc: 0.9827380952380952 \n",
            "\n",
            "loss: 1.475162148475647\n",
            "train acc: 0.9902229931526806\n",
            "eval acc: 0.9829761904761904 \n",
            "\n",
            "loss: 1.4727106094360352\n",
            "train acc: 0.9917982408216783\n",
            "eval acc: 0.9832142857142857 \n",
            "\n",
            "loss: 1.4708360433578491\n",
            "train acc: 0.9932255244755245\n",
            "eval acc: 0.9840476190476191 \n",
            "\n",
            "loss: 1.4693888425827026\n",
            "train acc: 0.9945116732226107\n",
            "eval acc: 0.9853571428571428 \n",
            "\n",
            "loss: 1.4682420492172241\n",
            "train acc: 0.9953766936188813\n",
            "eval acc: 0.9861904761904762 \n",
            "\n",
            "loss: 1.4673043489456177\n",
            "train acc: 0.9962052921037297\n",
            "eval acc: 0.9872619047619048 \n",
            "\n",
            "loss: 1.466511607170105\n",
            "train acc: 0.9968563337703964\n",
            "eval acc: 0.9872619047619048 \n",
            "\n",
            "loss: 1.4658770561218262\n",
            "train acc: 0.9973890042249418\n",
            "eval acc: 0.9877380952380952 \n",
            "\n",
            "loss: 1.4653682708740234\n",
            "train acc: 0.9975733901515151\n",
            "eval acc: 0.9877380952380952 \n",
            "\n",
            "loss: 1.4648900032043457\n",
            "train acc: 0.9980172821969697\n",
            "eval acc: 0.9863095238095239 \n",
            "\n",
            "loss: 1.4643757343292236\n",
            "train acc: 0.9981948390151515\n",
            "eval acc: 0.9870238095238095 \n",
            "\n",
            "loss: 1.4640374183654785\n",
            "train acc: 0.9985203598484849\n",
            "eval acc: 0.9880952380952381 \n",
            "\n",
            "loss: 1.4637515544891357\n",
            "train acc: 0.9986683238636364\n",
            "eval acc: 0.9876190476190476 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-7a702dac71d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-6bbf26c5df6b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3jULcpJ_yYg"
      },
      "source": [
        "model.eval()\n",
        "out = model(tensor_test)\n",
        "_, pred = torch.max(out.data, 1)\n",
        "print(pred.numpy().shape)\n",
        "\n",
        "submission = pd.read_csv(\"drive/MyDrive/Kaggle_digits/sample_submission.csv\")\n",
        "submission['Label'] = pred\n",
        "submission.to_csv(\"drive/MyDrive/Kaggle_digits/to_submit2.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMy5bpy8QBvk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}